{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from FirstModel import FirstModel\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 32, 32, 3)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 32, 32, 3)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (10000, 32, 32, 3)\n",
      "Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "from cs231n.data_utils import load_CIFAR10\n",
    "\n",
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=10000):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the two-layer neural net classifier. These are the same steps as\n",
    "    we used for the SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
    "# enc = OneHotEncoder()\n",
    "# y_train = enc.fit_transform(y_train.reshape(-1, 1))\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_out = FirstModel(32, 32, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no gpu found, please use Google Cloud if you want GPU acceleration\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with tf.Session() as sess:\n",
    "        with tf.device(\"/gpu:0\") as dev: #\"/cpu:0\" or \"/gpu:0\"\n",
    "            tf.global_variables_initializer().run()\n",
    "\n",
    "            ans = sess.run(y_out,feed_dict={X:x,is_training:True})\n",
    "            %timeit sess.run(y_out,feed_dict={X:x,is_training:True})\n",
    "except tf.errors.InvalidArgumentError:\n",
    "    print(\"no gpu found, please use Google Cloud if you want GPU acceleration\")    \n",
    "    # rebuild the graph\n",
    "    # trying to start a GPU throws an exception \n",
    "    # and also trashes the original graph\n",
    "    tf.reset_default_graph()\n",
    "#     X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "#     y = tf.placeholder(tf.int64, [None])\n",
    "#     is_training = tf.placeholder(tf.bool)\n",
    "#     y_out = FirstModel(X,y,is_training)\n",
    "    y_out = FirstModel(32, 32, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Iteration 0: with minibatch training loss = 2.62 and accuracy of 0.062\n",
      "Iteration 100: with minibatch training loss = 1.82 and accuracy of 0.38\n",
      "Iteration 200: with minibatch training loss = 1.7 and accuracy of 0.42\n",
      "Iteration 300: with minibatch training loss = 1.6 and accuracy of 0.39\n",
      "Iteration 400: with minibatch training loss = 1.56 and accuracy of 0.47\n",
      "Iteration 500: with minibatch training loss = 1.47 and accuracy of 0.52\n",
      "Iteration 600: with minibatch training loss = 1.62 and accuracy of 0.41\n",
      "Iteration 700: with minibatch training loss = 1.5 and accuracy of 0.47\n",
      "Epoch 1, Overall loss = 1.62 and accuracy of 0.441\n",
      "Iteration 800: with minibatch training loss = 1.52 and accuracy of 0.55\n",
      "Iteration 900: with minibatch training loss = 1.37 and accuracy of 0.53\n",
      "Iteration 1000: with minibatch training loss = 1.42 and accuracy of 0.58\n",
      "Iteration 1100: with minibatch training loss = 1.17 and accuracy of 0.64\n",
      "Iteration 1200: with minibatch training loss = 1.31 and accuracy of 0.61\n",
      "Iteration 1300: with minibatch training loss = 1.06 and accuracy of 0.66\n",
      "Iteration 1400: with minibatch training loss = 1.27 and accuracy of 0.62\n",
      "Iteration 1500: with minibatch training loss = 1.22 and accuracy of 0.62\n",
      "Epoch 2, Overall loss = 1.32 and accuracy of 0.574\n",
      "Iteration 1600: with minibatch training loss = 1.23 and accuracy of 0.61\n",
      "Iteration 1700: with minibatch training loss = 1.2 and accuracy of 0.66\n",
      "Iteration 1800: with minibatch training loss = 1.1 and accuracy of 0.67\n",
      "Iteration 1900: with minibatch training loss = 1.22 and accuracy of 0.59\n",
      "Iteration 2000: with minibatch training loss = 1.22 and accuracy of 0.62\n",
      "Iteration 2100: with minibatch training loss = 1.26 and accuracy of 0.62\n",
      "Iteration 2200: with minibatch training loss = 1.11 and accuracy of 0.62\n",
      "Epoch 3, Overall loss = 1.17 and accuracy of 0.636\n",
      "Iteration 2300: with minibatch training loss = 1.06 and accuracy of 0.7\n",
      "Iteration 2400: with minibatch training loss = 0.934 and accuracy of 0.8\n",
      "Iteration 2500: with minibatch training loss = 1.23 and accuracy of 0.58\n",
      "Iteration 2600: with minibatch training loss = 1.04 and accuracy of 0.67\n",
      "Iteration 2700: with minibatch training loss = 0.828 and accuracy of 0.81\n",
      "Iteration 2800: with minibatch training loss = 1.17 and accuracy of 0.59\n",
      "Iteration 2900: with minibatch training loss = 0.966 and accuracy of 0.72\n",
      "Iteration 3000: with minibatch training loss = 0.901 and accuracy of 0.73\n",
      "Epoch 4, Overall loss = 1.06 and accuracy of 0.681\n",
      "Iteration 3100: with minibatch training loss = 1.12 and accuracy of 0.56\n",
      "Iteration 3200: with minibatch training loss = 0.862 and accuracy of 0.73\n",
      "Iteration 3300: with minibatch training loss = 0.937 and accuracy of 0.72\n",
      "Iteration 3400: with minibatch training loss = 0.787 and accuracy of 0.8\n",
      "Iteration 3500: with minibatch training loss = 1.05 and accuracy of 0.66\n",
      "Iteration 3600: with minibatch training loss = 1 and accuracy of 0.67\n",
      "Iteration 3700: with minibatch training loss = 0.93 and accuracy of 0.72\n",
      "Iteration 3800: with minibatch training loss = 0.865 and accuracy of 0.73\n",
      "Epoch 5, Overall loss = 0.964 and accuracy of 0.716\n",
      "Iteration 3900: with minibatch training loss = 0.956 and accuracy of 0.78\n",
      "Iteration 4000: with minibatch training loss = 0.783 and accuracy of 0.81\n",
      "Iteration 4100: with minibatch training loss = 0.878 and accuracy of 0.75\n",
      "Iteration 4200: with minibatch training loss = 1.03 and accuracy of 0.72\n",
      "Iteration 4300: with minibatch training loss = 0.892 and accuracy of 0.72\n",
      "Iteration 4400: with minibatch training loss = 0.675 and accuracy of 0.84\n",
      "Iteration 4500: with minibatch training loss = 0.757 and accuracy of 0.81\n",
      "Epoch 6, Overall loss = 0.889 and accuracy of 0.741\n",
      "Iteration 4600: with minibatch training loss = 0.812 and accuracy of 0.78\n",
      "Iteration 4700: with minibatch training loss = 0.875 and accuracy of 0.66\n",
      "Iteration 4800: with minibatch training loss = 0.758 and accuracy of 0.84\n",
      "Iteration 4900: with minibatch training loss = 0.958 and accuracy of 0.7\n",
      "Iteration 5000: with minibatch training loss = 0.813 and accuracy of 0.8\n",
      "Iteration 5100: with minibatch training loss = 0.954 and accuracy of 0.75\n",
      "Iteration 5200: with minibatch training loss = 0.842 and accuracy of 0.73\n",
      "Iteration 5300: with minibatch training loss = 0.776 and accuracy of 0.77\n",
      "Epoch 7, Overall loss = 0.822 and accuracy of 0.764\n"
     ]
    }
   ],
   "source": [
    "y_out.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n",
      "Epoch 1, Overall loss = 0.865 and accuracy of 0.736\n"
     ]
    }
   ],
   "source": [
    "print('Validation')\n",
    "# run_model(sess,y_out,mean_loss,X_val,y_val,1,64)\n",
    "y_out.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Overall loss = 0.881 and accuracy of 0.738\n"
     ]
    }
   ],
   "source": [
    "y_out.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
